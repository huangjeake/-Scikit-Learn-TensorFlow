'''
LENET-5:
    就是一个或多个卷积层后面跟着一个池化层，然后又是若干个卷积层再接一个池化层，然后是全连接层，最后是输出，这种排列方式
    很常用,激活函数用的是sigmoid
AlexNet:
    神经网络与LeNet有很多相似之处，不过AlexNet要大得多,AlexNet比LeNet表现更为出色的另一个原因是它使用了ReLu激活函数
    AlexNet采用了非常复杂的方法在两个GPU上进行训练。大致原理是，这些层分别拆分到两个不同的GPU上，同时还专门有一个方法
    用于两个GPU进行交流。
VGG-16:
    VGG-16的这个数字16，就是指在这个网络中包含16个卷积层和全连接层。确实是个很大的网络，总共包含约1.38亿个参数，即便以
    现在的标准来看都算是非常大的网络。
    随着网络的加深，图像的高度和宽度都在以一定的规律不断缩小，每次池化后刚好缩小一半，而通道数量在不断增加，而且刚好也
    是在每组卷积操作后增加一倍。也就是说，图像缩小的比例和通道数增加的比例是有规律的。
ResNets:
    非常非常深的神经网络是很难训练的，因为存在梯度消失和梯度爆炸问题.在残差网络中有一点变化，我们将直接向后，拷贝到神
    经网络的深层，在ReLU非线性激活函数前加上，这是一条捷径。的信息直接到达神经网络的深层，不再沿着主路径传递
    对于一个普通网络来说，深度越深意味着用优化算法越难训练。实际上，随着网络深度的加深，训练错误会越来越多。
    使用残差网络性能不会受到影响，很多时候甚至可以提高效率，或者说至少不会降低网络的效率，因此创建类似残差网络可以提升
    网络性能
Inception network：
    将普通的卷积层替换称为卷积组合。
    将池化层替换成1 * 1的卷积层组合或者将5 * 5的卷积层替换成两个3 * 3的卷积层，可以降低参数数目和计算量，又可以不影响性能

参数数量：
    1.1 卷积网络
        假设卷积核的大小为 k*k, 输入channel为M， 输出channel为N。
        （1）bias为True时：
        则参数数量为：k×k×M×N + N（bias的数量与输出channel的数量是一样的）
        （2）bias为False时：
        则参数数量为：k×k×M×N
    1.2 全连接层
        假设 输入神经元数为M，输出神经元数为N，则
        （1）bias为True时：
        则参数数量为：M*N + N（bias的数量与输出神经元数的数量是一样的）
        （2）bias为False时：
        则参数数量为：M×N
计算量：
    2.1 卷积
        假设输入特征图（B，C，H，W），卷积核大小为K×K， 输入通道为C，输出通道为N，步长stride为S， 输出特征图大小为H2，W2.
        （1）一次卷积的计算量
        一个k×k的卷积，执行一次卷积操作，需要k×k次乘法操作（卷积核中每个参数都要和特征图上的元素相乘一次），k×k−1 次加
        法操作（将卷积结果，k×k 个数加起来）。所以，一次卷积操作需要的乘加次数：(K×K)+(K×K−1)=2×K×K−1
        （2）在一个特征图上需要执行卷积需要卷积的次数
        在一个特征图上需要执行的卷积次数：(（H-k+Ph）/S +1 )×(（H-k+Pw）/S +1)，Ph，Pw表示在高和宽方向填充的像素，此处
        假定了宽高方向滑动步长和核的宽高是一样，若不同，调整一下值即可。若不能整除，可向下取整。
    2.1 全连接
        假设 输入神经元数为M，输出神经元数为N，则
        （1）先执行M次乘法；
        （2）再执行M-1次加法
        （3）加上bias，计算出一个神经元的计算量为 （M+M-1+1）
        （4）N个输出神经元，则总的计算量为 2M×N


1×1卷积层就是这样实现了一些重要功能的（doing something pretty non-trivial），它给神经网络添加了一个非线性函数，从而减少
或保持输入层中的通道数量不变


计算机视觉：
    数据扩充方法：
    第一种镜像对称和随机裁剪是经常被使用的。当然，理论上，你也可以使用旋转，剪切（shearing：此处并非裁剪的含义，图像仅
    水平或垂直坐标发生变化）图像，可以对图像进行这样的扭曲变形，引入很多形式的局部弯曲等等。当然使用这些方法并没有坏处，
    尽管在实践中，因为太复杂了所以使用的很少。
    第二种经常使用的方法是彩色转换。
    最后常用的实现数据扩充的方法是使用一个线程或者是多线程，这些可以用来加载数据，实现变形失真，然后传给其他的线程或者
    其他进程，来训练这个（编号2）和这个（编号1），可以并行实现。

对象定位
特征点检测
目标检测
卷积网络实现滑动窗口对象检测算法，但效率很低
YOLO(you only look once)算法用于计算机视觉，这个算法的优点在于神经网络可以输出精确的边界框YOLO算法有一个好处，也是它
受欢迎的原因，因为这是一个卷积实现，实际上它的运行速度非常快，可以达到实时识别

并交比可以评价对象检测算法
一般约定，在计算机检测任务中，如果loU > 0.5，就说检测正确，如果预测器和实际边界框完美重叠，loU就是1，因为交集就等于并集。
但一般来说只要loU > 0.5，那么结果是可以接受的，看起来还可以。一般约定，0.5是阈值，用来判断预测的边界框是否正确。一般是这
么约定，但如果你希望更严格一点，你可以将loU定得更高，比如说大于0.6或者更大的数字，但loU越高，边界框越精确
loU衡量了两个边界框重叠地相对大小

非极大值抑制这个方法可以确保你的算法对每个对象只检测一次,非最大值意味着你只输出概率最大的分类结果，但抑制很接近，但不是
最大的其他预测结果,根据交并比抑制其他矩形，所以这方法叫做非极大值抑制。

anchor box：可以让一个格子检测出多个对象

R-CNN：选出一些候选区域，然后对候选区域进行分类


'''